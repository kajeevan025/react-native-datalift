#!/usr/bin/env bash
# ::: Code Generated by Copilot [6f1a2e2d-3a4f-4d5b-9c2e-7d8f9a0b1c2d]. This comment will be removed automatically after the file is saved :::

set -euo pipefail

ROOT_DIR="$(cd "$(dirname "$0")/.." && pwd)"
TARGET_DIR="${ROOT_DIR}/assets/models/layoutlmv3-base"
MODEL_REPO="microsoft/layoutlmv3-base"

echo "[DataLift] Preparing offline LayoutLMv3 model assets..."
mkdir -p "${TARGET_DIR}"

if command -v huggingface-cli >/dev/null 2>&1; then
  echo "[DataLift] Using huggingface-cli download for ${MODEL_REPO}"
  huggingface-cli download "${MODEL_REPO}" \
    --local-dir "${TARGET_DIR}" \
    --local-dir-use-symlinks False
  echo "[DataLift] Model files downloaded to: ${TARGET_DIR}"
  exit 0
fi

if ! command -v git >/dev/null 2>&1; then
  echo "[DataLift] ERROR: neither huggingface-cli nor git found."
  echo "Install one of them and re-run this script."
  exit 1
fi

if ! command -v git-lfs >/dev/null 2>&1; then
  echo "[DataLift] ERROR: git-lfs is required for large model files."
  echo "Install git-lfs and run: git lfs install"
  exit 1
fi

git lfs install
TMP_DIR="${TARGET_DIR}/.tmp-clone"
rm -rf "${TMP_DIR}"

echo "[DataLift] Cloning ${MODEL_REPO} via git-lfs..."
git clone "https://huggingface.co/${MODEL_REPO}" "${TMP_DIR}"

# Keep only files needed for offline inference packaging
find "${TMP_DIR}" -maxdepth 1 -type f \( \
  -name "config.json" -o \
  -name "tokenizer.json" -o \
  -name "tokenizer_config.json" -o \
  -name "vocab.json" -o \
  -name "merges.txt" -o \
  -name "special_tokens_map.json" -o \
  -name "preprocessor_config.json" -o \
  -name "*.safetensors" -o \
  -name "*.onnx" -o \
  -name "*.bin" \
\) -exec cp -f {} "${TARGET_DIR}" \;

rm -rf "${TMP_DIR}"

echo "[DataLift] Offline model assets ready in: ${TARGET_DIR}"
echo "[DataLift] Next: implement your native on-device runner (CoreML / ONNX Runtime Mobile) and pass it to HuggingFaceProvider({ runner })."
